
# Design an API Server to serve your algorithm

In this section we will talk about the difficulties and rules pertaining to the creation of an API Server.

The creation of an API Server for your algorithms (more accurately the versions of algorithms) is a very important step when creating your algorithms on AlgoFab platform because there are RESTfull communications on all the different entities of AlgoFab your algorithm included. Thus you will have to develop a RESTfull server wrapped around your algorithm in order to handle all incoming requests.


<aside class="warning">
	This section will be heavily related to Kubernetes, if you did not, finish learning the bases of Kubernetes  before you continue reading.
</aside>

Let's give an example of algorithm first and form that derive the generalities an contraints of AlgoFab. 

Suppose my algorithm is about finding the shortest path (Dijkstra's algorithm) on an evolving (new nodes can be added or deleted, same goes for vertexes) graph recorded inside a database. I also created a cache in order to record the responses for a short period of time so that i don't have to repeat the same calculations over and over again without remembering.

Suppose that - to create such an algorithm - I decided to use a MONGO DB database to store the graph, a Redis Server to serve as a cache and an HTTP server to serve as API server. In this case we will have 3 Pods kubernetes and at least two services to make it work. The three Pods will obviously be the database (db), the cache (redis) and the API (api). The two services are used to expose the Pod **db** (let's call **db-svc**) and the Pod **redis** (let's call **redis-svc**) so the Pod **api** can use both Pods. As it is from the Pod **api** you can make your call and everything will work fine. However, I will have to create another service to expose the API Server to the users (let's call **api-svc**).

Suppose all the infrastructure is created, which service should I consider each time a request is addressed to this algorithm? Should I prepare for both Demonstration and API modes? 

These are the two important point from which we'll dervie the contraints.

## Unique entry point principle :

For the sake of simplicity for both contributors and users we suppose on Algofab that your algorithm has only one entry point and that mean that the algorithm has one and only on the communicate outside the Kubernetes Cluster. The Kubernetes Service that represents that link is called **main_service** on Algofab. 

<aside class="notice">
	The <b>main_service</b> is the service that exposes your algorithm to the end users and through which all requests for processing pass. 
</aside> 

In the example above, the main_service, that will allow me to conform to that "unique entry point principle" should be **api-svc**. The general rule is that the service that exposes the API Server is the main_service and that API server is implemented to serve whatever functionnality the algorithm should have. So in our case the main_service has to be a **NodePort** type of service (out of **NodePort** and **LoadBalancer** to expose the services, AlgoFab chose **NodePort**) while the others are **ClusterIP** type of services.

<aside class="notice">
	In the manifest of a version, the main_service is always of type NodePort and other services are ClusterIP types. When analysing the manifest if the Portal does not see any specified main_service, or the specified main_service is not a NodePort type of Service, the creation process will abort and you'll be prompted with the corresponding error message.
</aside> 

**What about having multiple ports for the main_service?**

This question raises the same the issue as previously except we have multiple ports now instead of having multiple services, therefore the conclusion is the same. We will make it possible eventually in the furture to expose multiple ports in the main_service's spec, but as for now it is not allowed.

## Demonstration mode VS API mode  

On Algofab we assume all contributors want to promote their algorithms. We then came up with the Demonstration mode and the API mode.

* The demonstration mode is a graphical mode (HTML) prepared and served by the API Server in order to showcase the efficiency of the algorithm, hence promoting and convincing other users to turn to it for their needs. 
* API mode is prepared by the API Server in order to treat the requests and return the results to the user assuming he will use it in his external application. The difference with the demonstration mode is that we don't need any fancy GUI but rather the format (xml, json, yaml ...) of the response so it is exploitable in the client's own application.

The demonstration mode is just for presenting and the algorithm and the API mode is the actual fullfilment of the purpose of the algorithm : do a subtask for a client in his general application.

The problem however is that we only have one Service as entry point and moreover only one port in this service's specification. You can do anything you want to resolve that issue (using a reverse proxy - towards two other pods one for demonstration mode and the other for API mode - as API server for example). As for the HelloWorld algorithm in the Getting Started tutorial, we created a query parameter **outformat** which can be xml, json or html by default. 

In Any case you should make sure that whenever a **GET** is made on the root URL ("/") of your API Server, we access the Demonstration mode of your algorithm. Whenever a user try to access a live demo of your algorithm on the Portal, the Portal just acts as a reerse proxy and sends a **"GET /"** to the API Server (Wether the contributor was prepared or not for that).

<aside class="notice">
	You can just ignore the demonstration mode if you think there is no use to it (in that case do not forget to mention it in the field <b>comment</b> of the version's manifest). But in case you want to support it, remember they will be served by the same server and the demo should be available at "GET /" on your API Server. 
</aside>








